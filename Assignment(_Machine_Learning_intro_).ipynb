{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Intro | Assignment**"
      ],
      "metadata": {
        "id": "U5qnX-y_qaPI"
      },
      "id": "U5qnX-y_qaPI"
    },
    {
      "cell_type": "markdown",
      "id": "30fd9fdd",
      "metadata": {
        "id": "30fd9fdd"
      },
      "source": [
        "## Q.1 : Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS)\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- **Artificial Intelligence (AI):** The broad field that aims to create machines or systems that can perform tasks that normally require human intelligence (e.g., reasoning, planning, language understanding). Example: a chatbot that answers customer queries.\n",
        "\n",
        "- **Machine Learning (ML):** A subset of AI that trains algorithms on data so they can make predictions or decisions without being explicitly programmed for every rule. Example: a model that predicts house prices from historical data.\n",
        "\n",
        "- **Deep Learning (DL):** A subset of ML that uses multi-layered neural networks (deep neural networks) to learn complex patterns from very large datasets. DL often requires more data and computation. Example: image recognition (identifying objects in photos).\n",
        "\n",
        "- **Data Science (DS):** An interdisciplinary field that uses statistics, programming, domain knowledge, and ML to extract insights and build data-driven solutions. Data science includes data cleaning, exploration, visualization, modeling, and communication. Example: analyzing customer purchase data to recommend products.\n",
        "\n",
        "**Short relation:** AI ⊇ ML ⊇ DL. Data Science uses ML/DL and also covers data preparation, analysis, and communication.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e6ad62",
      "metadata": {
        "id": "d6e6ad62"
      },
      "source": [
        "## Q.2 : What are the types of machine learning? Describe each with one real-world example\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Supervised Learning:** Trains on labelled data (inputs paired with correct outputs). Used for classification and regression.\n",
        "   - *Example:* Email spam detection (labels: spam / not spam).\n",
        "\n",
        "2. **Unsupervised Learning:** Finds patterns in unlabelled data (no explicit output labels). Used for clustering, dimensionality reduction.\n",
        "   - *Example:* Customer segmentation in marketing (group customers by behavior).\n",
        "\n",
        "3. **Semi-supervised Learning:** Uses a small amount of labelled data plus a larger amount of unlabelled data to improve learning.\n",
        "   - *Example:* Web page classification where only a few pages are labelled but many are unlabelled.\n",
        "\n",
        "4. **Reinforcement Learning (RL):** An agent learns by taking actions in an environment to maximize cumulative reward.\n",
        "   - *Example:* Training a robot to navigate a maze or training agents for game-playing (AlphaGo).\n",
        "\n",
        "5. **Self-supervised Learning (emerging / specialised):** The model creates labels from the data itself (e.g., predict missing words) to learn useful representations.\n",
        "   - *Example:* BERT-style pretraining in NLP (predict masked words) used before fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f393c2",
      "metadata": {
        "id": "08f393c2"
      },
      "source": [
        "## Q.3 : Define overfitting, underfitting, and the bias-variance tradeoff in machine learning\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- **Overfitting:** When a model learns the training data too well, including noise and minor fluctuations, it performs very well on training data but poorly on new/unseen data. It often happens with overly complex models and small data.\n",
        "  - *Fixes:* Use simpler models, regularization, more data, cross-validation, or early stopping.\n",
        "\n",
        "- **Underfitting:** When a model is too simple to capture the underlying pattern in the data and performs poorly on both training and test data.\n",
        "  - *Fixes:* Use a more complex model, add relevant features, reduce regularization.\n",
        "\n",
        "- **Bias-Variance Tradeoff:** Bias measures error from wrong assumptions (underfitting). Variance measures sensitivity to training data (overfitting). The tradeoff is balancing these two so that total error (bias^2 + variance + irreducible error) is minimized. Simple models → high bias, low variance. Complex models → low bias, high variance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de6bb77",
      "metadata": {
        "id": "9de6bb77"
      },
      "source": [
        "## Q.4 : What are outliers in a dataset, and list three common techniques for handling them.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- **Outliers:** Data points that differ significantly from other observations. They can come from measurement errors, data-entry mistakes, or true extreme behaviour.\n",
        "\n",
        "**Three common techniques:**\n",
        "1. **Remove outliers:** If outliers are errors or irrelevant, drop them. Be careful with small datasets.\n",
        "2. **Cap/Winsorize:** Replace extreme values with boundary values (e.g., set values above 95th percentile to the 95th percentile value).\n",
        "3. **Transform data:** Apply log or Box–Cox transforms to reduce skew and the effect of extreme values.\n",
        "\n",
        "Other approaches: use robust models (median, robust scaling) or treat separately as a special case.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7231de08",
      "metadata": {
        "id": "7231de08"
      },
      "source": [
        "## Q.5 : Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Process:**\n",
        "1. **Identify** missing values and compute the percentage missing per column.\n",
        "2. **Understand missingness type:** MCAR (missing completely at random), MAR (missing at random), MNAR (not at random).\n",
        "3. **Decide strategy:** Drop rows/columns (if too many missing), impute, or model the missingness.\n",
        "4. **Impute/handle:** Apply chosen methods (mean/median/mode, regression imputation, KNN, or domain-specific rules).\n",
        "5. **Validate:** Check results and whether imputations introduce bias; test model sensitivity.\n",
        "\n",
        "**One imputation technique (numerical):** **Median imputation** — replace missing numeric values with the median (robust to outliers).\n",
        "\n",
        "**One imputation technique (categorical):** **Mode imputation** — replace missing categorical values with the most frequent category.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd2d85e9",
      "metadata": {
        "id": "bd2d85e9"
      },
      "source": [
        "### Q.6 : Python code (create imbalanced dataset and print class distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3279adb",
      "metadata": {
        "id": "f3279adb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q6: Create a synthetic imbalanced dataset with make_classification and print class distribution.\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=0,\n",
        "                           n_clusters_per_class=1, weights=[0.95, 0.05], flip_y=0, random_state=42)\n",
        "\n",
        "df_q6 = pd.DataFrame(X, columns=[f'feat_{i+1}' for i in range(X.shape[1])])\n",
        "df_q6['target'] = y\n",
        "\n",
        "print('Q6: Class distribution (label: count):')\n",
        "print(Counter(df_q6['target']))\n",
        "\n",
        "print('\\\\nQ6: Sample rows:')\n",
        "print(df_q6.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b886b93",
      "metadata": {
        "id": "3b886b93"
      },
      "source": [
        "### Q.7 : Python code (one-hot encoding using pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002989bb",
      "metadata": {
        "id": "002989bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q7: One-hot encoding using pandas for a list of colors ['Red', 'Green', 'Blue', 'Green', 'Red'].\n",
        "import pandas as pd\n",
        "\n",
        "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "df_colors = pd.DataFrame({'color': colors})\n",
        "\n",
        "print('Q7: Original dataframe:')\n",
        "print(df_colors)\n",
        "\n",
        "df_onehot = pd.get_dummies(df_colors, columns=['color'], prefix='', prefix_sep='')\n",
        "# reorder columns to consistent order\n",
        "cols = sorted([c for c in df_onehot.columns if c != 'color']) if 'color' in df_onehot.columns else sorted(df_onehot.columns)\n",
        "df_onehot = df_onehot[cols] if cols else df_onehot\n",
        "\n",
        "print('\\\\nQ7: One-hot encoded dataframe:')\n",
        "print(df_onehot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39416a75",
      "metadata": {
        "id": "39416a75"
      },
      "source": [
        "### Q.8 : Python code (generate normal samples, introduce missing values, impute with mean, plot histograms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f6dcca",
      "metadata": {
        "id": "a1f6dcca"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q8: Generate 1000 samples from a normal distribution, introduce 50 random missing values, fill with mean, and plot histograms before and after imputation.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "values = np.random.normal(loc=50, scale=10, size=1000)  # mean 50, sd 10\n",
        "df_q8 = pd.DataFrame({'value': values})\n",
        "\n",
        "# introduce 50 random missing values\n",
        "missing_idx = np.random.choice(df_q8.index, size=50, replace=False)\n",
        "df_q8.loc[missing_idx, 'value'] = np.nan\n",
        "\n",
        "print('Q8: Missing values count before imputation:', df_q8['value'].isna().sum())\n",
        "\n",
        "# Histogram BEFORE imputation\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(df_q8['value'].dropna(), bins=30)\n",
        "plt.title('Q8: Histogram BEFORE imputation (missing removed)')\n",
        "plt.xlabel('value')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()\n",
        "\n",
        "# Fill missing with column mean\n",
        "mean_val = df_q8['value'].mean()\n",
        "df_q8['value'] = df_q8['value'].fillna(mean_val)\n",
        "print('Q8: Missing values count after imputation:', df_q8['value'].isna().sum())\n",
        "\n",
        "# Histogram AFTER imputation (separate plot)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(df_q8['value'], bins=30)\n",
        "plt.title('Q8: Histogram AFTER imputation (filled with mean)')\n",
        "plt.xlabel('value')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9a4ca9",
      "metadata": {
        "id": "4e9a4ca9"
      },
      "source": [
        "### Q.9 : Python code (Min-Max scaling using sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf000ba",
      "metadata": {
        "id": "4bf000ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q9: Implement Min-Max scaling on [2,5,10,15,20] using sklearn.preprocessing.MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "arr = np.array([2, 5, 10, 15, 20]).reshape(-1,1)\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(arr)\n",
        "print('Q9: Original array:', arr.flatten().tolist())\n",
        "print('Q9: Scaled array:', scaled.flatten().tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87083b0f",
      "metadata": {
        "id": "87083b0f"
      },
      "source": [
        "## Q.10 : Data preparation plan for a retail customer transaction dataset (step-by-step)\n",
        "\n",
        "**Scenario:** Dataset contains missing ages, outliers in transaction amount, highly imbalanced target (fraud vs non-fraud), and categorical variables like payment method.\n",
        "\n",
        "**Step-by-step plan (simple and practical):**\n",
        "\n",
        "1. **Initial exploration:** Examine column types, missing value counts, basic statistics (mean, median, min, max) and target class distribution.\n",
        "\n",
        "2. **Missing data handling:**\n",
        "   - For **age (numerical)**: check distribution; if not heavily skewed use mean/median imputation. Prefer **median** if outliers present.\n",
        "   - For **categorical (payment method)**: impute with **mode** (most frequent) or add a special category like 'Missing'.\n",
        "\n",
        "3. **Outliers in transaction amount:**\n",
        "   - Detect with IQR or z-score. If outliers are data errors, remove them. If they are valid but extreme, consider capping (winsorizing) or transforming (log transform) to reduce impact.\n",
        "\n",
        "4. **Encoding categorical variables:**\n",
        "   - Use **one-hot encoding** for nominal categories with limited cardinality (e.g., payment_method).\n",
        "   - For high-cardinality categories, consider target-encoding or embedding approaches.\n",
        "\n",
        "5. **Handle class imbalance:**\n",
        "   - Options: oversample minority (upsampling), undersample majority, or use algorithmic methods (class weights, specialized algorithms, or synthetic sampling like SMOTE).\n",
        "   - A simple approach: **upsample minority** using resampling or set `class_weight` in models like RandomForest/LogisticRegression.\n",
        "\n",
        "6. **Feature scaling:**\n",
        "   - Scale numeric features (StandardScaler or MinMaxScaler) where required (e.g., for distance-based models or regularized models).\n",
        "\n",
        "7. **Feature engineering & selection:**\n",
        "   - Create derived features (e.g., transaction_hour, user_age_group), and remove/reduce redundant features.\n",
        "\n",
        "8. **Validation strategy:**\n",
        "   - Use stratified train-test split to preserve class imbalance in validation. Use cross-validation (stratified) and monitor metrics appropriate for imbalance (precision, recall, F1, ROC-AUC, PR-AUC).\n",
        "\n",
        "9. **Pipeline & reproducibility:**\n",
        "   - Build an automated pipeline (sklearn Pipeline) that handles imputation, encoding, scaling, and sampling to avoid data leakage.\n",
        "\n",
        "10. **Final checks:**\n",
        "   - Evaluate models using relevant metrics and perform post-processing (threshold tuning) before deployment.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b485934",
      "metadata": {
        "id": "8b485934"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Q10: Demonstration of data preparation steps for a retail transaction dataset.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "n = 500\n",
        "\n",
        "# Create synthetic data\n",
        "ages = np.random.normal(loc=35, scale=12, size=n).astype(float)\n",
        "# Introduce missing ages (10% missing)\n",
        "missing_age_idx = np.random.choice(n, size=int(0.10*n), replace=False)\n",
        "ages[missing_age_idx] = np.nan\n",
        "\n",
        "# Transaction amount: mostly around 100-500 but with some outliers\n",
        "tx_amount = np.random.exponential(scale=100, size=n) + 20  # positive skew\n",
        "# Introduce some extreme outliers\n",
        "outlier_idx = np.random.choice(n, size=5, replace=False)\n",
        "tx_amount[outlier_idx] = tx_amount[outlier_idx] * 15\n",
        "\n",
        "# Target: highly imbalanced (fraud = 0.03)\n",
        "targets = np.zeros(n, dtype=int)\n",
        "fraud_idx = np.random.choice(n, size=int(0.03*n), replace=False)\n",
        "targets[fraud_idx] = 1\n",
        "\n",
        "# Payment method categorical with some missing values\n",
        "payment_methods = np.random.choice(['Card', 'Cash', 'UPI', 'NetBanking'], size=n, p=[0.45, 0.25, 0.25, 0.05])\n",
        "# introduce a few missing payment methods\n",
        "pm_missing_idx = np.random.choice(n, size=int(0.03*n), replace=False)\n",
        "payment_methods[pm_missing_idx] = None\n",
        "\n",
        "df_q10 = pd.DataFrame({\n",
        "    'age': ages,\n",
        "    'transaction_amount': tx_amount,\n",
        "    'payment_method': payment_methods,\n",
        "    'is_fraud': targets\n",
        "})\n",
        "\n",
        "print('Q10: Initial dataset shape and first rows:')\n",
        "print(df_q10.shape)\n",
        "print(df_q10.head())\n",
        "\n",
        "# 1) Initial exploration\n",
        "print('\\\\nQ10: Missing values per column:')\n",
        "print(df_q10.isna().sum())\n",
        "\n",
        "print('\\\\nQ10: Transaction amount summary (before handling outliers):')\n",
        "print(df_q10['transaction_amount'].describe())\n",
        "\n",
        "print('\\\\nQ10: Target distribution:')\n",
        "print(df_q10[\"is_fraud\"].value_counts())\n",
        "\n",
        "# 2) Handling missing data\n",
        "# Age: median imputation\n",
        "age_median = df_q10['age'].median()\n",
        "df_q10['age_imputed'] = df_q10['age'].fillna(age_median)\n",
        "\n",
        "# Payment method: mode imputation, if no mode available set to 'Missing'\n",
        "mode_pm = df_q10['payment_method'].mode()\n",
        "mode_pm_val = mode_pm[0] if len(mode_pm)>0 else 'Missing'\n",
        "df_q10['payment_method_imputed'] = df_q10['payment_method'].fillna(mode_pm_val)\n",
        "\n",
        "print('\\\\nQ10: After imputation - missing counts:')\n",
        "print(df_q10[['age_imputed','payment_method_imputed']].isna().sum())\n",
        "\n",
        "# 3) Outlier detection & handling for transaction_amount using IQR capping (winsorizing)\n",
        "Q1 = df_q10['transaction_amount'].quantile(0.25)\n",
        "Q3 = df_q10['transaction_amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "print(f'\\\\nQ10: IQR lower_bound={lower_bound:.2f}, upper_bound={upper_bound:.2f}')\n",
        "\n",
        "# Count outliers\n",
        "outliers_lower = (df_q10['transaction_amount'] < lower_bound).sum()\n",
        "outliers_upper = (df_q10['transaction_amount'] > upper_bound).sum()\n",
        "print('Q10: Outliers lower count:', outliers_lower)\n",
        "print('Q10: Outliers upper count:', outliers_upper)\n",
        "\n",
        "# Cap values to bounds (winsorize)\n",
        "df_q10['tx_capped'] = df_q10['transaction_amount'].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print('\\\\nQ10: Transaction amount summary (after capping):')\n",
        "print(df_q10['tx_capped'].describe())\n",
        "\n",
        "# 4) Encoding categorical variable - one-hot encoding for payment_method_imputed\n",
        "df_encoded = pd.get_dummies(df_q10, columns=['payment_method_imputed'], prefix='pm')\n",
        "\n",
        "# 5) Handle class imbalance - upsample minority class (fraud) to balance\n",
        "df_majority = df_encoded[df_encoded['is_fraud']==0]\n",
        "df_minority = df_encoded[df_encoded['is_fraud']==1]\n",
        "print('\\\\nQ10: Before resampling class distribution:')\n",
        "print(df_encoded['is_fraud'].value_counts())\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print('Q10: After upsampling class distribution:')\n",
        "print(df_balanced['is_fraud'].value_counts())\n",
        "\n",
        "# 6) Feature scaling (StandardScaler) for numeric features 'age_imputed' and 'tx_capped'\n",
        "scaler = StandardScaler()\n",
        "df_balanced[['age_scaled', 'tx_scaled']] = scaler.fit_transform(df_balanced[['age_imputed', 'tx_capped']])\n",
        "\n",
        "print('\\\\nQ10: Final prepared dataset shape and sample rows:')\n",
        "print(df_balanced.shape)\n",
        "print(df_balanced[['age_imputed','tx_capped','age_scaled','tx_scaled','is_fraud'] + [c for c in df_balanced.columns if c.startswith('pm_')]].head())\n",
        "\n",
        "# Try to use caas_jupyter_tools.display_dataframe_to_user if available for nicer display\n",
        "try:\n",
        "    import caas_jupyter_tools as cjt\n",
        "    cjt.display_dataframe_to_user('Q10_sample_prepared', df_balanced.head(10))\n",
        "except Exception:\n",
        "    print('\\\\nNote: caas_jupyter_tools not available, used printed output instead.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}